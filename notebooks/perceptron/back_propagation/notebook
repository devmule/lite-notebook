%% md

# Обратное распространение ошибки


___



Не будем мять кота за тити, приступим сразу же к реализации обратного распространения ошибки.

В теоретической части мы получили такую формулу:

$$ \frac{ \Delta Loss }{\Delta W} = 2( \hat{y} - y ) * σ'(z_i) * y_{i-1},$$

Эта формула описывает зависимость между значением функции ошибки **Loss** и весами для данной пары входных-выходных значений.

В этой формуле есть производная от функции сигмоиды, значит нам нужно реализовать её.
Заодно вспомним саму функцию сигмоиды и функцию прямого распространения. Они нам пригодятся!


%% fetch
text: sigmoid_txt = sigmoid.py
text: backprop_txt = backprop.py
text: learn_txt = learn.py
text: test_txt = test.py

%% code id=editor_sigmoid run
%% md
### Теперь само обратное распространение ошибки

Вот очень советую сравнить то что происходит в коде и в формуле. Может тогда формула понятнее станет)

Обучение у нас проходит по типу "пакетного метода". Это легко реализовать с помощью `numpy` и многомерных матриц - **тензоров**.

%% code id=editor_backprop run
%% md
### Ок, это один проход. Но ведь их нужно несколько?
Да! Для полного обучения выборке сделаем отдельную функцию. В ней будут всякие условия для завершения обучения:

- максимальное количество итераций
- завершение по достижению определённой точности

Позже в эту функцию можно добавить улучшения, например **"mini-batching"** - **мини-пакетный** метод.

%% code id=editor_learn run
%% md

___

# Протестируем реализацию!

В начале курса я рассказывал что Мински и Паперт доказали что персептрон не может реализовать функцию
[**"XOR"**](https://ru.wikipedia.org/wiki/исключающее_или), а он на самом деле может!

Они рассматривали персептрон без скрытых слоёв!

Прикол в том, что персептрон без скрытых слоёв нейронов (с одним слоем синапсов) представляет из себя линейную функцию!
Функции **"И"**, **"ИЛИ"** линейно разделимы, а вот **"XOR"** - нет!

![and or xor](and_or_xor.jpg?style=center)

Персептроны же со скрытыми слоями более умные, понимают нелинейные зависимости :)

В примере ниже я подготовил персептрон с одним скрытым слоем. Он может обучиться **"XOR"**.

А вот если оставить только один слой синапсов `2 нейрона -> 1 нейрон`,
то персептрон станет линейным и не сможет обучиться :) Попробуй обучить!

%% code id=editor_test run
%% js
    var editor_sigmoid = ace.edit("editor_sigmoid");
    editor_sigmoid.getSession().setValue(sigmoid_txt);
    editor_sigmoid.setOptions({maxLines: 60, minLines: 14});

    var editor_backprop = ace.edit("editor_backprop");
    editor_backprop.getSession().setValue(backprop_txt);

    var editor_learn = ace.edit("editor_learn");
    editor_learn.getSession().setValue(learn_txt);

    var editor_test = ace.edit("editor_test");
    editor_test.getSession().setValue(test_txt);
    let test_code_output = lite_notebook.elements.code_output();
    lite_notebook.screen.appendChild(test_code_output);
    test_code_output.attachEditor(editor_test);
    test_code_output.before_launch = () => {
        test_code_output.pre_codes = [
            editor_sigmoid.getValue(),
            editor_backprop.getValue(),
            editor_learn.getValue(),
        ];
    }


%% md