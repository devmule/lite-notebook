>>> md

# Улучшение: Мини-пакеты


___


В теоретической статье я писал что видов градиентного спуска бывает несколько:
- **Стохастический** (его еще иногда называют онлайн) метод работает по следующему принципу —
нашел Δw для одного элемента выборки, сразу обнови соответствующий вес.
- **Пакетный** метод же работает по другому. Мы суммируем Δw всех весов на текущей итерации
и только потом обновляем все веса используя эту сумму. Один из самых важных плюсов
такого подхода — это значительная экономия времени на вычисление, точность же в таком случае может пострадать.
- **Мини-пакетный** метод является золотой серединой и пытается совместить в себе плюсы
обоих методов. Здесь принцип таков: мы в свободном порядке распределяем веса
по группам и меняем их веса на сумму Δw всех весов в той или иной группе.


В нашем случае реализован **пакетный** метод градиентного спуска. Чтобы сделать его мини-пакетным,
нужно в функции `learn` разбить пакеты на несколько мини-пакетов, и в одной итерации пропустить через функцию `backprop`
все мини-пакеты.

Это хитрая реализация :) хитрая потому, что если задать `mini_batch_len <= 0`, то обучение будет работать как
**Пакетный** метод. А если задать `mini_batch_len = 1`, то будет работать как **Стохастический** метод, т.к. будет
делать проход для каждого элемента выборки отдельно. В других же случаях она будет работать как **Мини-пакетный** метод.

>>> fetch
text: mb_txt = mb.py

>>> code id=mini_batch
>>> js
    var mini_batch = ace.edit("mini_batch");
    mini_batch.getSession().setValue(mb_txt);